{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14242,"databundleVersionId":568274,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Loading","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.pipeline import Pipeline, FunctionTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import roc_auc_score\nimport mlflow\nimport mlflow.sklearn\nimport dagshub\n\ndagshub.init(repo_owner='gnada22', repo_name='ml_assignment_2', mlflow=True)\n\nmlflow.set_experiment(\"AdaBoost_Training\")\n\ntrain_identity = pd.read_csv('/kaggle/input/ieee-fraud-detection/train_identity.csv')\ntrain_transaction = pd.read_csv('/kaggle/input/ieee-fraud-detection/train_transaction.csv')\n\ntrain = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\n\nfeatures = [col for col in train.columns if col not in [\"TransactionID\", \"isFraud\"]]\nX = train[features]\ny = train[\"isFraud\"]\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Cleaning","metadata":{}},{"cell_type":"code","source":"with mlflow.start_run(run_name=\"AdaBoost_Cleaning\"):\n    threshold = 0.7\n    missing_ratio = X_train.isnull().mean()\n    cols_to_drop = missing_ratio[missing_ratio > threshold].index.tolist()\n    X_train.drop(columns=cols_to_drop, inplace=True)\n    X_valid.drop(columns=cols_to_drop, inplace=True)\n    mlflow.log_metric(\"initial_missing_ratio\", missing_ratio.mean())\n    mlflow.log_param(\"drop_threshold\", threshold)\n    mlflow.log_param(\"num_cols_dropped\", len(cols_to_drop))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Feature Engineering","metadata":{}},{"cell_type":"code","source":"numerical_cols = X_train.select_dtypes(include=np.number).columns.tolist()\ncategorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n\nwith mlflow.start_run(run_name=\"AdaBoost_Feature_Engineering\"):\n    def add_transaction_amt_log(X):\n        X = X.copy()\n        if \"TransactionAmt\" in X.columns:\n            X[\"TransactionAmt_log\"] = np.log1p(X[\"TransactionAmt\"])\n        return X\n\n    log_transformer = FunctionTransformer(add_transaction_amt_log, validate=False)\n\n    numerical_transformer = Pipeline([\n        (\"imputer\", SimpleImputer(strategy=\"median\")),\n        (\"scaler\", StandardScaler())\n    ])\n\n    categorical_transformer = Pipeline([\n        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n    ])\n\n    if \"TransactionAmt\" in numerical_cols:\n        mlflow.log_param(\"new_features\", \"TransactionAmt_log\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Feature Selection","metadata":{}},{"cell_type":"code","source":"with mlflow.start_run(run_name=\"AdaBoost_Feature_Selection\"):\n    k_best = 50\n    selector = SelectKBest(score_func=f_classif, k=k_best)\n    selector.fit(X_train[numerical_cols].fillna(0), y_train)\n    selected_features = np.array(numerical_cols)[selector.get_support()].tolist()\n\n    if \"TransactionAmt\" in selected_features:\n        selected_features.append(\"TransactionAmt_log\")\n    \n    mlflow.log_param(\"num_selected_features\", len(selected_features))\n    mlflow.log_param(\"selected_features\", \", \".join(selected_features))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Training","metadata":{}},{"cell_type":"code","source":"with mlflow.start_run(run_name=\"AdaBoost_Model_Training\"):\n    preprocessor = Pipeline([\n        (\"log\", log_transformer),\n        (\"column_transform\", ColumnTransformer([\n            (\"num\", numerical_transformer, selected_features),\n            (\"cat\", categorical_transformer, categorical_cols)\n        ]))\n    ])\n\n    ab = AdaBoostClassifier(random_state=42)\n\n    pipeline = Pipeline([\n        (\"preprocessor\", preprocessor),\n        (\"classifier\", ab)\n    ])\n\n    param_grid = {\n        'classifier__n_estimators': [50, 100, 200],\n        'classifier__learning_rate': [0.5, 1.0, 1.5]\n    }\n\n    grid_search = GridSearchCV(pipeline, param_grid, scoring='roc_auc', cv=3, n_jobs=-1)\n    grid_search.fit(X_train, y_train)\n\n    best_model = grid_search.best_estimator_\n    best_params = grid_search.best_params_\n\n    y_train_pred = best_model.predict_proba(X_train)[:, 1]\n    y_valid_pred = best_model.predict_proba(X_valid)[:, 1]\n\n    train_auc = roc_auc_score(y_train, y_train_pred)\n    valid_auc = roc_auc_score(y_valid, y_valid_pred)\n\n    mlflow.log_metrics({\"train_auc\": train_auc, \"valid_auc\": valid_auc})\n    mlflow.log_params(best_params)\n\n    mlflow.sklearn.log_model(best_model, \"fraud_pipeline\")\n\n    print(f\"Train AUC: {train_auc:.4f}\")\n    print(f\"Validation AUC: {valid_auc:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}